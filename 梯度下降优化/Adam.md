# 深度学习中的 Adam 笔记

## 1. 引言

**Adam**（Adaptive Moment Estimation）是一种广泛应用于深度学习中的优化算法，它结合了[**动量法**](Momentum.md)（加速收敛） 和[**AdaGrad**](Adagrad.md)（自适应学习率） 的优点，能够更好地处理稀疏梯度和非平稳目标。Adam 在许多深度学习任务中表现出色，尤其是在大规模数据集和高维参数空间中。

## 2. Adam 的原理

Adam 算法的核心思想是使用梯度的一阶矩（均值）和二阶矩（方差）来动态调整每个参数的学习率。它结合了动量法和自适应学习率的优点，使得每个参数在训练过程中拥有自己的学习率，并能适应不同梯度的变化。

Adam 的参数更新过程分为两个阶段：一阶矩和二阶矩的估计。

### 1. 一阶矩估计（动量）

一阶矩估计是梯度的指数加权平均，表示当前梯度的“动量”。计算公式如下：

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

其中：
- $m_t$ 是当前时刻梯度的一阶矩估计；
- $g_t$ 是当前时刻的梯度；
- $\beta_1$ 是一阶矩的衰减率，通常设置为接近 1（如 0.9）。

### 2. 二阶矩估计（梯度的平方）

二阶矩估计是梯度平方的指数加权平均，用于衡量梯度的方差。计算公式如下：

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

其中：
- $v_t$ 是当前时刻梯度的二阶矩估计；
- $\beta_2$ 是二阶矩的衰减率，通常设置为接近 1（如 0.999）。

### 3. 偏差修正

由于一阶矩和二阶矩的估计在初期时会偏向零，Adam 引入了偏差修正步骤：
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
这里，$\hat{m}_t$ 和 $\hat{v}_t$ 是经过偏差修正的估计值。

### 4. 参数更新

最终，参数更新的公式为：

$$
\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

其中：
- $\theta_t$ 是当前时刻的参数；
- $\alpha$ 是学习率；
- $\epsilon$ 是防止除零的小常数（通常设置为 $10^{-8}$）。

## 3. Adam 的优缺点

### 优点

- **自适应学习率**：Adam 根据每个参数的梯度一阶矩和二阶矩自适应调整学习率，使得训练过程更加稳定且快速。

- **适应稀疏梯度**：通过自适应学习率，Adam 特别适合处理稀疏梯度的问题，常见于自然语言处理和计算机视觉任务。

- **较少的调参**：相比于其他优化算法，Adam 需要调节的超参数较少（主要是学习率、$\beta_1$ 和 $\beta_2$）。

- **偏差修正**：通过引入偏差修正，Adam 可以在训练的早期阶段保持较好的更新效率。

### 缺点

- **可能收敛到局部最优**：尽管 Adam 能加速收敛，但它也可能停留在局部最优解而非全局最优解，特别是在非凸优化问题中。

- **参数选择敏感**：虽然 Adam 对超参数不太敏感，但如果 $\beta_1$ 和 $\beta_2$ 设置不当，仍然可能影响训练效果。

- **计算成本较高**：Adam 需要存储一阶矩和二阶矩的估计，对于大规模模型和数据集，计算和内存开销较高。

## 4. 总结

Adam 是一种非常有效的优化算法，它结合了动量法和自适应学习率的优点，适用于各种深度学习任务。它通过对梯度的均值和方差进行自适应调整，能够在训练过程中稳定且快速地收敛。然而，Adam 仍然受到局部最优解和超参数选择的影响，因此在某些任务中，可能需要与其他方法结合使用来进一步提升性能。
