# Leaky ReLU 笔记

## 原理

Leaky ReLU（泄露线性修正单元）是ReLU激活函数的一种变体，旨在解决ReLU中存在的“死亡神经元”问题——即当输入为负时，ReLU函数的输出总是0，这可能导致某些神经元不再学习。Leaky ReLU通过允许负值以一个小斜率传递来缓解这个问题。其数学表达式如下：
$$f(x) = \left\{
  \begin{array}{ll}
    x & \text{if } x \geq 0 \\
    \alpha x & \text{if } x < 0
  \end{array}
\right.$$
其中，$\alpha$ 是一个很小的正数（如0.01），决定了负数部分的倾斜程度。
![](LeakyReLU.png)

## 梯度更新

Leaky ReLU 的导数（梯度）在正值区域为1，在负值区域为$\alpha$。这种特性使得即使对于负值输入，梯度也不会完全消失，从而有助于避免“死亡神经元”问题，并促进更稳定的训练过程。其导数表达式如下：
$$ f'(x) = \left\{
  \begin{array}{ll}
    1 & \text{if } x > 0 \\
    \alpha & \text{if } x < 0
  \end{array}
\right. $$
注意：在$x=0$时，通常需要根据具体实现指定$f'(x)$的值。

## 优缺点

### 优点

- **解决了“死亡神经元”问题**：由于在负数区域保持了一个小斜率，即使对于负值输入，梯度也能继续流动。

- **计算效率高**：和ReLU一样，Leaky ReLU的计算非常简单快速。

### 缺点

- **超参数调整**：需要选择合适的$\alpha$值，这可能对最终性能有影响。

- **不一定总比ReLU好**：尽管解决了ReLU的一些问题，但在某些情况下，Leaky ReLU并不一定能带来显著的性能提升。