## 一、原理

**ReLU（Rectified Linear Unit）** 是一种广泛应用的激活函数，其数学表达式为：
$$f(x)=\max(0,x)$$
这意味着对于输入x，如果x大于0，则输出x；如果x小于或等于0，则输出0。
![](ReLUDiagram.png)
## 二、梯度更新

ReLU的导数为：
- 当$x>0$时，$f'(x)=1$
- 当$x<0$时，$f'(x)=0$
- 在$x=0$处，根据具体实现可能有所不同，但通常认为导数是0或者1

在反向传播过程中，只有当输入大于0时，梯度才会被传递回去，这有助于加速训练过程，因为相比sigmoid和tanh激活函数，它避免了梯度消失的问题。

![](ReLUBackPropagation.drawio.svg)
## 三、优缺点
### 优点

- **计算效率高**：由于其实现简单，只需比较操作，因此计算速度非常快。

- **缓解梯度消失问题**：在正区间内保持梯度不变，有助于加速深层网络的训练。

- **稀疏激活性**：可以产生真正的零值，有助于网络的稀疏表示。

### 缺点

- **死亡ReLU问题**：如果输入持续为负，ReLU单元将停止学习，因为它们的梯度总是0，导致神经元“死亡”。

- **非零中心化**：ReLU的输出不是以零为中心的，这可能会影响训练效果。

总的来说，ReLU因其高效性和实用性，在现代深度学习模型中得到了广泛的应用。然而，使用ReLU时也需要注意其潜在的“死亡”问题，并考虑采用变种如Leaky ReLU来缓解这一问题。